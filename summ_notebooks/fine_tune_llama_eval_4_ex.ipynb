{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sru7d5r8JyfD",
        "outputId": "6db3f12a-3589-4659-d7f3-b142a7ebbc3a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hf_token=userdata.get('hf_token')\n",
        "\n",
        "!pip install -q huggingface_hub\n",
        "from huggingface_hub import login\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "Saw32hwfM5mx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q transformers peft bitsandbytes accelerate\n",
        "#!pip install -q datasets trl wandb evaluate rouge_score bert_score sacrebleu sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEHhfdnzNHRA",
        "outputId": "5a4bd086-5652-4d2d-ec9f-1d178416a6ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_1ouPpkH39_",
        "outputId": "22347a28-88b8-44f7-f9e6-5c8386b2186e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-30 20:43:42.266205: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-30 20:43:42.283572: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746045822.304981   15867 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746045822.311442   15867 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-30 20:43:42.333304: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "data: /content/drive/MyDrive/DL4H-Project/mimic-iv-note-di-bhc/dataset\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaveenkcb\u001b[0m (\u001b[33mnaveenkcb-university-of-illinois-urbana-champaign\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/DL4H-Project/patient_summaries_with_llms/wandb/run-20250430_204347-16zi69cm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcrimson-sun-16\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/naveenkcb-university-of-illinois-urbana-champaign/mimic-iv-note-di-bhc_Llama-2-7b-hf_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/naveenkcb-university-of-illinois-urbana-champaign/mimic-iv-note-di-bhc_Llama-2-7b-hf_evaluation/runs/16zi69cm\u001b[0m\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n",
            "Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/model.safetensors.index.json\n",
            "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "target_dtype {target_dtype} is replaced by `torch.int8` for 8-bit BnB quantization\n",
            "Loading checkpoint shards: 100% 2/2 [00:15<00:00,  7.86s/it]\n",
            "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 4096,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Loading data from {'train': '/content/drive/MyDrive/DL4H-Project/mimic-iv-note-di-bhc/dataset/train_4000_600_chars_251-350_pt.json', 'validation': '/content/drive/MyDrive/DL4H-Project/mimic-iv-note-di-bhc/dataset/valid_4000_600_chars.json', 'test': '/content/drive/MyDrive/DL4H-Project/mimic-iv-note-di-bhc/dataset/valid_4000_600_chars.json'}\n",
            "Number of training examples: 8\n",
            "Number of validation examples: 4\n",
            "Number of test examples: 4\n",
            "Truncated texts: 0 train, 0 val, 0 test\n",
            "Prompted train example:\n",
            "Summarize for the patient what happened during the hospital stay based on this doctor's note:\n",
            "Brief Hospital Course: NEUROLOGICAL COURSE IN ICU AND ON THE GENERAL SERVICE: Mrs. ___ was initially admitted to the stroke service given her presenting deficits and concern regarding acute brainstem infarction. Within 24 hours, she was transfered to the ICU as her exam deteriorated. With regards to her cranial nerve deterioration, she became completely ophthalmoplegic and could not deviate her gaze from midline at all. She had no oculocephalic reflex. She had facial diplegia, lingual dysarthria, and complete lack of palatal elevation requiring dobhoff placement as she could not swallow. Mental status and vision were unaffected. Her mobility was also affected to the point where she was only minimally able to wiggle her toes and fingers. During this time, she continued to have reflexes of patella and biceps. Her intial lumbar puncture and MRI were unremarkable. These were repeated three days later on ___, to assess for interval change and to broaden work-up. EMG was done which was more consistent with CNS than PNS process. The work-up resulted positive for GQ1b positive antibody profile consistent with ___ brainstem encephalitis which has a good response to IVIG therapy. She received 5 days of Solumedrol and then 5 days of IVIG from ___ to ___. She continued to progressively improve with return of full palatal function and facial movement and on discharge was able to eat unassisted. Her strength improved greatly by discharge to the physical exam that is profiled in the previous section. Of note on discharge, she continues to have complete opthalmoplegia. She will likely continue to improve without additional treatment and we will assess need for further treatment at her outpatient Neurology appointment in ___. It is important, however, to alert her listed Neurologist if there is evidence that her neurological exam is declining. Other non-neurological issues during this hospitalization: 1) Uncontrolled hypertension as she was not taking her prescribed meds prior to admission. She was started on amlodipine, metoprolol (for h/o STEMI) and HCTZ. 2) Diabetes: poorly controlled diabetes for which ___ was consulted. She was discharged with Lantus 20mg QAm with Humalog sliding scale with past 48hrs being 100-210. 3) CT torso showed breast nodularity, however this was felt to be negative for malignancy. 4) Aerococcal UTI s/p Vancomycin ___. 5) Left wrist pain: xray done, no acute fracture 6) Right great toe painful, will require podiatric attention TRANSITIONAL ISSUES: - Bladder scanned for 220 on discharge. Would reccomend monitoring for urinary retention. - Full code - Please contact her Neurologist if there is concern of declining neurological exam. - Please arrange podiatry consultation for right great toe.\n",
            "Summary for the patient: You were admitted with weakness, instability while walking, and double vision. While you were here, you had many tests including bloodwork, imaging of your brain and chest, spinal tap, EEG (brainwave study) and studies of the nerves in your muscles (EMG). The results of these tests showed that your symptoms were caused by an autoimmune process. More specifically, your GQ1b antibody level was positive which indicates that you had a ___ brainstem encephalitis that is known to cause difficulty with eye movement and weakness. This syndrome is closely associated with guillain ___ syndrome. We have treated you with steroids and IVIG and you are continuing to recover. Our physical therapists felt that you would benefit from further intensive physical therapy at ___. </s> \n",
            "\n",
            "Prompted valid example:\n",
            "Summarize for the patient what happened during the hospital stay based on this doctor's note:\n",
            "Brief Hospital Course: Mr ___ by ___ is a ___ male with h/o CKD, HTN, aortic dissection (type B), G6PD deficiency, OSA, eczema, who presents with left leg tenderness, edema, and warmth for 5 days, consistent with cellulitis, re-presenting s/p abdominal distress after clindamycin. . #) Cellulitis: Presented with GI distress ___ clindamycin, so should not take that again. Pt does not have diabetes, which narrows likely organisms needed to cover. Choice of abx was complicated by G6PD deficiency. Got vancomycin in the ER.Sent home on 7 day course of augmentin. Remained afebrile and non-septic throughout admission. . #) ___ on CKD: baseline fluctates but around 4. CKD is likely ___ poorly controlled HTN. Came in with creatinine 4.8. ___ likely pre-renal ___ nausea and diarrhea from clindamycin. Was repleted with IVF, and creatinine came back to 4.0. He is seen by Dr ___ in nephrology, and is overdue for a check-up, so pt advised to call on ___ to make an appointment. . #) HTN, malignant: continued home dose of amlodipine, labetolol, and minoxidil . #) Anemia: baseline in mid 30___s. ___ be progression of CKD. Iron studies suggestive of anemia of chronic disease. . #) Depression: poorly controlled, although strongly denies any SI or thought of hurting self. Wants to restart citalopram and see a psychiatrist. Defered restarting SSRI to person who will follow it up. Gave number for getting a psychiatric appointment. . #) Smoking: ___ packs per day, given smoking cessation education\n",
            "Summary for the patient: You were admitted with a leg infection (called cellulitis) and because your kidney function had worsened. For the leg infection, you were treated with an antibiotic (called augmentin), and you will take this for 5 more days. Your kidney function was decreased because of dehydration, likely from nausea and vomiting the day before, and had improved back to baseline when you were discharged. Because of your high blood pressure, prior aortic dissection, and chronic kidney disease, you are at high risk for heart disease. The single biggest thing you can do to decrease your risk of dying from a heart attack is to quit smoking. </s> \n",
            "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "Loading model for evaluation: /content/drive/MyDrive/DL4H-Project/mimic-iv-note-di-bhc/models/Llama-2-7b-hf/mimic-iv-note-di-bhc_Llama-2-7b-hf_4000_600_chars_100_valid/lora_rank_8_lora_alpha_8_lora_dropout_0.05_num_target_modules_2_learning_rate_2e-5/best_val_loss\n",
            "Starting prediction debugging test...\n",
            "Input length: 504 tokens\n",
            "GPU memory before generation: 6.56GB / 8.96GB\n",
            "Starting generation for one example...\n",
            "/content/drive/MyDrive/DL4H-Project/patient_summaries_with_llms/summarization/fine_tune_llama_3.py:429: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Generation complete in 44.81 seconds\n",
            "GPU memory after generation: 6.57GB / 8.96GB\n",
            "\n",
            "Sample prediction:\n",
            "Gold summary: You were admitted with a leg infection (called cellulitis) and because your kidney function had worsened. For the leg infection, you were treated with an antibiotic (called augmentin), and you will take this for 5 more days. Your kidney function was decreased because of dehydration, likely from nausea and vomiting the day before, and had improved back to baseline when you were discharged. Because of your high blood pressure, prior aortic dissection, and chronic kidney disease, you are at high risk for heart disease. The single biggest thing you can do to decrease your risk of dying from a heart attack is to quit smoking.\n",
            "Model prediction: Mr ___ was admitted with cellulitis of his left leg. He was treated with vancomycin in the ER, and was discharged home on 7 days of augmentin. He has been doing well, and is now on his way home. \n",
            "# 51\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "#\n",
            "\n",
            "Estimated time for 4 examples: 2.99 minutes\n",
            "\n",
            "The full prediction will take significant time. Do you want to continue? (y/n)\n",
            "y\n",
            "Continuing with full prediction...\n",
            "\n",
            "Predicting test examples.\n",
            "/content/drive/MyDrive/DL4H-Project/patient_summaries_with_llms/summarization/fine_tune_llama_3.py:293: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "prediction for 4 examples done...\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 110kB/s]\n",
            "config.json: 100% 482/482 [00:00<00:00, 1.69MB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 3.49MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 19.2MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 38.5MB/s]\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/merges.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer_config.json\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 1.42G/1.42G [00:02<00:00, 516MB/s]\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/model.safetensors\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "tokenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 296kB/s]\n",
            "config.json: 100% 729/729 [00:00<00:00, 3.56MB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-large-mnli/snapshots/7296194b9009373def4f7c5dad292651e4b5cf4e/config.json\n",
            "Model config DebertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"DebertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"CONTRADICTION\",\n",
            "    \"1\": \"NEUTRAL\",\n",
            "    \"2\": \"ENTAILMENT\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"CONTRADICTION\": 0,\n",
            "    \"ENTAILMENT\": 2,\n",
            "    \"NEUTRAL\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"legacy\": true,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"c2p\",\n",
            "    \"p2c\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"relative_attention\": true,\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 39.2MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 28.6MB/s]\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-large-mnli/snapshots/7296194b9009373def4f7c5dad292651e4b5cf4e/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-large-mnli/snapshots/7296194b9009373def4f7c5dad292651e4b5cf4e/merges.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-large-mnli/snapshots/7296194b9009373def4f7c5dad292651e4b5cf4e/tokenizer_config.json\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-large-mnli/snapshots/7296194b9009373def4f7c5dad292651e4b5cf4e/config.json\n",
            "Model config DebertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"DebertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"CONTRADICTION\",\n",
            "    \"1\": \"NEUTRAL\",\n",
            "    \"2\": \"ENTAILMENT\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"CONTRADICTION\": 0,\n",
            "    \"ENTAILMENT\": 2,\n",
            "    \"NEUTRAL\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"legacy\": true,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"c2p\",\n",
            "    \"p2c\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"relative_attention\": true,\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-large-mnli/snapshots/7296194b9009373def4f7c5dad292651e4b5cf4e/config.json\n",
            "Model config DebertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"DebertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"CONTRADICTION\",\n",
            "    \"1\": \"NEUTRAL\",\n",
            "    \"2\": \"ENTAILMENT\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"CONTRADICTION\": 0,\n",
            "    \"ENTAILMENT\": 2,\n",
            "    \"NEUTRAL\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"legacy\": true,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"c2p\",\n",
            "    \"p2c\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"relative_attention\": true,\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "pytorch_model.bin: 100% 1.62G/1.62G [00:04<00:00, 352MB/s]\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-large-mnli/snapshots/7296194b9009373def4f7c5dad292651e4b5cf4e/pytorch_model.bin\n",
            "Attempting to create safetensors variant\n",
            "Attempting to convert .bin model on the fly to safetensors.\n",
            "All the weights of DebertaModel were initialized from the model checkpoint at microsoft/deberta-large-mnli.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaModel for predictions without further training.\n",
            "Test examples:\n",
            "\n",
            "\n",
            "\n",
            "Example 0:\n",
            "\n",
            "Brief Hospital Course: Mr ___ by ___ is a ___ male with h/o CKD, HTN, aortic dissection (type B), G6PD deficiency, OSA, eczema, who presents with left leg tenderness, edema, and warmth for 5 days, consistent with cellulitis, re-presenting s/p abdominal distress after clindamycin. . #) Cellulitis: Presented with GI distress ___ clindamycin, so should not take that again. Pt does not have diabetes, which narrows likely organisms needed to cover. Choice of abx was complicated by G6PD deficiency. Got vancomycin in the ER.Sent home on 7 day course of augmentin. Remained afebrile and non-septic throughout admission. . #) ___ on CKD: baseline fluctates but around 4. CKD is likely ___ poorly controlled HTN. Came in with creatinine 4.8. ___ likely pre-renal ___ nausea and diarrhea from clindamycin. Was repleted with IVF, and creatinine came back to 4.0. He is seen by Dr ___ in nephrology, and is overdue for a check-up, so pt advised to call on ___ to make an appointment. . #) HTN, malignant: continued home dose of amlodipine, labetolol, and minoxidil . #) Anemia: baseline in mid 30___s. ___ be progression of CKD. Iron studies suggestive of anemia of chronic disease. . #) Depression: poorly controlled, although strongly denies any SI or thought of hurting self. Wants to restart citalopram and see a psychiatrist. Defered restarting SSRI to person who will follow it up. Gave number for getting a psychiatric appointment. . #) Smoking: ___ packs per day, given smoking cessation education\n",
            "\n",
            "You were admitted with a leg infection (called cellulitis) and because your kidney function had worsened. For the leg infection, you were treated with an antibiotic (called augmentin), and you will take this for 5 more days. Your kidney function was decreased because of dehydration, likely from nausea and vomiting the day before, and had improved back to baseline when you were discharged. Because of your high blood pressure, prior aortic dissection, and chronic kidney disease, you are at high risk for heart disease. The single biggest thing you can do to decrease your risk of dying from a heart attack is to quit smoking.\n",
            "\n",
            "You were admitted to the hospital with cellulitis, a bacterial skin infection. You were treated with antibiotics and sent home.\n",
            "You were also admitted for ____.\n",
            "Your ___ is ___.\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "#) ___\n",
            "\n",
            "\n",
            "\n",
            "Example 1:\n",
            "\n",
            "Brief Hospital Course: ___ RHM h/o T2-T8 abscess s/p laminectomy, T6 Zoster flare presented with recent worsening of L>RLE weakness and sensory loss concerning for association with VZV-related myelopathy, either due to active disease or possibly autoimmune response ___ VZV. On admission examination, the patient demonstrated more profound weakness in the bilateral lower extremities with worse LLE than previously reported. Sensation was also decreased predominantly vibration and proprioception at the hallux. Graphesthesia was intact throughout. Reflexes were brisk bilaterally in ___ with crossed adductors and extensor toe reflex. He was admitted and started on high dose steroids (IV methylprednisolone for five days) and Valacyclovir. MRI ___ demonstrated enhancement in thoracic spine consistent with level of prior Zoster. He was noted to have an elevated pulse during admission (P100s) which was thought secondary to autonomic dysfunction caused by his spinal cord inflammatory process. This was controlled with Metoprolol 12.5mg BID, which can be tapered after completion of prednisone (i.e. resolution of acute illness). He was also noted to have elevated blood sugars (BG 200s) while on high dose steroids. This was controlled with sliding scale insulin. His symptoms improved greatly during his admission. After the five day course, he was discharged on oral Prednisone (80mg Qday), Valacyclovir (1000mg TID) and Metoprolol (12.5 BID). Upon discharge, he was instructed that his hyperglycemia would likely resolve once off the high dose steroids. He was prescribed a glucometer/test strips in order to monitor his sugars at home. He was instructed to follow up with his PCP as an outpatient if his hyperglycemia persisted. He was also educated about possible stomach upset and ulcers while on steroids. He was instructed to continue to take GI prophylaxis, Ranitidine 150mg BID, while on oral steroids. Close follow up was arranged with Dr ___ on ___ at 2pm. Dr ___ has agreed to manage the prednisone taper. He will need to continue to take Valacyclovir 1000mg every eight hours for two weeks after stopping prednisone.\n",
            "\n",
            "D@ ___, You were hospitalized with symptoms of left leg weakness. We attribute this weakness to a myelopathy secondary to herpes zoster infection. We started ___ on high dose steroids (IV methylprednisolone for five days) and Valacyclovir. Your symptoms improved greatly with this treatment. We plan to discharge ___ on oral Prednisone, Valacyclovir and Metoprolol. We have scheduled ___ with close follow up in Dr ___ ___ on ___ at 2pm. One side effect of steroids is high blood sugar. This may not be a problem as we are discharging ___ with a much lower steroid dose. We have ordered ___ a glucometer to routinely check your blood sugars. Please check in the morning (fasting) and before bedtime.\n",
            "\n",
            "You were admitted to the hospital with a severe flare of your ___ ___ ___ disease. You had a recent worsening of your lower extremity weakness and sensory loss. We believe that this was caused by an active ___ ___ ___ flare. You were treated with high dose steroids and ___ ___ ___ (a ___ ___ ___ antiviral medication). Your symptoms improved greatly during your hospital stay. You were discharged on ___ ___ ___ and ___ ___ ___ . You will need to follow up with your PCP as an outpatient.\n",
            "\n",
            "\n",
            "\n",
            "Example 2:\n",
            "\n",
            "Brief Hospital Course: Mr. ___ presented with inability to handle his secretions, weak cough and generalized weakness for three days. On initial examination he was weak in his extremities, with preserved neck flexion and extension and PFTs. He was found to have a left lower lobe infiltrate on chest x-ray concerning for pneumonia and he was started on levofloxacin. In the ED he developed worsening cough and dyspnea and on repeat assessment had significant neck flexor and extensor weakness and decreasing NIF and so he was admitted to the ICU. He received BiPAP overnight and his antibiotics were stopped due to the concern that they were contributing to his myasthenia crisis. His strength and respiration improved and he was transferred out of the ICU. Antibiotics were transitioned to ceftriaxone, for which he was treated with a full course. While on the floor, a tunneled pheresis catheter was placed, and he received 5 treatments with plasmapheresis. His mestinon was initially increased to 60mg qid, but this caused an increase in secretions. As such, it was titrated down to 45mg qid with a prn glycopyrrolate if he has an increase in secretions. NIFs/VC stable for >1 week (-50/1.3 on ___. He continued to have diarrhea even after antibiotics were stopped. His tube feeds were changed, and banana flakes were added. C diff was negative. Diarrhea likely ___ antibiotics, takes several weeks to resolve. At time of discharge, it had slowed down. Immodium helps, but did increase K to 6 for which he was given insulin and Lasix, so immodium can be used sparingly. CT chest showed bronchial inflammation, which may be ___ silent aspiration. Video swallow was done and showed mild oral pharyngeal dysphagia; so Speech and Swallow recommended: a. PO diet: soft, moist solids with thin liquids b. Meds whole in applesauce c. oral care BID d. Aspiration precautions -slow rate -single sips -cease intake with excess fatigue -alternate sips with every ___ bites - upright for all PO intake He had several episodes of stool with frank red blood, which were attributed to hemorrhoids. His CBC remained stable. Buttock Ulcer was evaluated by Wound Care who recommended TID walking. Dermatology evaluated and felt it likely skin breakdown in the setting of chronic irritant dermatitis but resolved HSV vs Strep ulceration is also on the DDX. Recommended bacterial and viral swab (preliminary negative). Also recommended mupirocin ointment to erosions BID as well as pressure relief. If bacterial and viral cultures are negative, can start triamcinolone 0.1% ointment BID to affected areas on gluteal clefts for up to 2 weeks in a row, then take 7 days off. Transitional Issues: - Continue 60 mg prednisone until followup - Neurology Follow-up on ___ ___ Dr. ___ - ___ or PCP consult for lower extremity edema - F/u Skin Scrapings, if negative can start triamcinolone as above - Start Probiotics at rehab for diarrhea\n",
            "\n",
            "As you know, you were recently admitted to the ___ Neurology Service for worsening weakness, slurred speech, and difficulty swallowing. This is due to your myasthenia ___, which was likely worsened in the setting of pneumonia from aspiration. While you were here, we treated you for your chronic medical conditions with your usual home doses of medications. We also treated you with antibiotics for an infection in your lungs (pneumonia). We monitored you for your respiratory function closely. You had some diarrhea because of the antibiotics, which can last for several weeks. It was slowing down at the time of your discharge. You make take immodium sparingly for this. For your myasthenia ___, we continued your prednisone, cell cept and changed your dose of mestinon to 45mg 4 times per day, and you had five sessions of plasmapharesis to treat this disease flare. You were also started on glycopyrrolate to help with the diarrhea. We stopped the glycopyrrolate once the diarrhea slowed down and started you on immodium. Throughout your hospitalization, your symptoms improved dramatically.\n",
            "\n",
            "Mr. ___ was admitted to the hospital with pneumonia and myasthenia crisis. He was treated with antibiotics and plasmapheresis, and his symptoms improved. He was discharged with instructions to continue his prednisone. He is also to continue with his oral care and wound care as needed. \n",
            "# _12_\n",
            "#\n",
            "The day after the admission, the patient's wife called me. She was very upset. \"Dr. ___ told me that I need to take my husband to the emergency room if he has a fever.\" I explained that fever was not a good way to monitor the patient. I asked if he had a cough, shortness of breath, or any other symptoms. She said no. \"He has no symptoms.\" I asked if he was eating, drinking, and going to the bathroom. She said he was eating and drinking but had had diarrhea. I asked if he was urinating. She said yes, but only a little. \"I think he has a urinary tract infection.\" She said she would call me back if he had any symptoms.\n",
            "The patient was seen by the internist. He was noted to have a fever of 100.3¬∞F. He was given Tylenol and discharged.\n",
            "I called the internist. He said he thought the patient had a urinary tract infection. I told him that I had discussed this with the patient's wife and that he was discharged home. He said, \"Well, if he has a fever\n",
            "\n",
            "\n",
            "\n",
            "Example 3:\n",
            "\n",
            "Brief Hospital Course: ___ ___ male with h/o Alzheimer's dementia, nonverbal at baseline, DM2, HTN, CRI, and PVD who presented with mental status changes most likely secondary to infectious etiology given fever and CT findings suggestive of possible colitis and PNA. Also noted to have hypernatremia, likely secondary to dehydration/decreased PO intake. . #Altered Mental Status: The patient initially presented to the ED after he was found to be more lethargic by his family. His mental status changes were felt to be secondary to infectious etiology, given fever, leukocytosis, and imaging findings of possible pneumonia and colitis on presentation. His mental status improved with antibiotics and IVFs, and he was back to his baseline mental status prior to discharge. Of note, he is generally non-verbal at baseline, with occasional mumbled speech. Per his family, he will respond to hearing his name, but does not follow commands. . #Colitis: Patient presented with diffuse abdominal tenderness on exam, and CT abdomen/pelvis findings were suggestive of colitis (infectious vs. inflammatory vs. ischemic) in the descending colon. Stool was guiac positive. Lactate was within normal limits. Given fever and leukocytosis in addition to exam and imaging findings, the patient was started on antibiotic therapy. He had received vanco/zosyn and metronidazole initially in ED, and was switched to metronidazole/levofloxacin on transfer to floor given concern for colitis and CAP. Following initiation of antibiotics, the patient remained afebrile, WBC trended down, and abdominal tenderness resolved. The patient will be discharged on 4 additional days of PO metronizadole to complete a 10 day course. . #Pneumonia: Chest imaging on presentation could not exclude consolidation, and patient was noted to have new oxygen requirement, fever, and leukocytosis on day of admission. Was some concern for aspiration PNA given dementia, though speech and swallow evaluation felt patient did not appear to be aspirating. Patient started on levofloxacin given concern for community-acquired PNA, and completed a five day course of treatment. He was afebrile for the remainder of his hospital course, WBC trended down, and he was satting well on room air at time of discharge. . #Hypernatremia: Na level noted to be elevated at 157 on admission, and hypernatremia was most likely secondary to dehydration in setting of decreased PO intake and dementia. Na level trended down with IVF administration, and was within normal limits at 144 prior to discharge. The patient's family was advised to encourage increased fluid intake at home following discharge. The patient will have repeat labs 3 days post-discharge to monitor Na levels, and he also has PCP ___ 5 days post-discharge. . #DM Type 2: The patient was continued on home dose of insulin glargine 40 units QAM, with insulin s/s. He had several episodes of hyperglycemia with FSBS in the 300s-400s, in setting of infection and administration of IVF with ___ NS to correct hypernatremia. His insulin s/s was adjusted accordingly. The patient should ___ with his PCP regarding possible adjustment of his insulin dosing at home. . #Hypertension: Per family's report, the patient was not on any anti-hypertensives prior to admission. During his hospital course, he was noted to have elevated BPs, with SBP ranging 130s-170s. He was briefly started on amlodipine 2.5mg PO daily, however this was not continued on discharge given desire to simply medication regimen in setting of advanced dementia. The patient will ___ with his PCP regarding his hypertension. . #Acute on Chronic Kidney Injury: Cr 2.2 on admission, elevated above baseline of around 1.4. ___ likely pre-renal azotemia in setting of decreased PO intake. The patient's Cr trended down to baseline with continued IVF administration, and was stable at 1.3 prior to discharge. . #. Code Status: The patient's code status was full code during this admission.\n",
            "\n",
            "You were admitted to the hospital after you were noted to be very lethargic. We found your sodium levels were very high, and this is likely because you were dehydrated. Your sodium levels improved with IV fluids, but it is VERY important that you drink lots of fluids at home. We also found you may have an infection in your intestine, and this condition is called colitis. We treated you with antibiotics, and you improved. You will need to continue taking antibiotics for 4 more days after you leave the hospital. We also treated you for a pneumonia while you were here. You have completed the course of antibiotics to treat the pneumonia.\n",
            "\n",
            "You were admitted to the hospital with mental status changes that were most likely secondary to an infectious etiology. You were given antibiotics and IV fluids to treat these changes, and your mental status improved. You were also noted to have hypernatremia, which is likely secondary to dehydration. Your family was advised to encourage increased fluid intake at home to prevent this from happening again. You also had some abdominal pain, and were given antibiotics to treat this. You were discharged on 4 additional days of PO metronidazole to complete a 10 day course of treatment.\n",
            "Test metrics:\n",
            "defaultdict(<class 'list'>, {'rouge1': np.float64(34.806234608887564), 'rouge2': np.float64(10.449193415447718), 'rouge3': np.float64(3.7532321984782184), 'rouge4': np.float64(1.8182813355227148), 'rougeL': np.float64(22.310177143896762), 'words': np.float64(126.75), 'bert_score': np.float64(83.23255479335785), 'bert_score_deberta-large': np.float64(52.45757922530174), 'sari': 41.32421379381164})\n",
            "Test metrics rounded:\n",
            "{'rouge1': np.float64(34.81), 'rouge2': np.float64(10.45), 'rouge3': np.float64(3.75), 'rouge4': np.float64(1.82), 'rougeL': np.float64(22.31), 'words': np.float64(126.75), 'bert_score': np.float64(83.23), 'bert_score_deberta-large': np.float64(52.46), 'sari': 41.32}\n",
            "$34.81$ & $10.45$ & $3.75$ & $1.82$ & $22.31$ & $83.23$ & $52.46$ & $41.32$ & $126.75$\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading wandb-summary.json 249B/249B (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading config.yaml 2.2KB/2.2KB (0.4s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading wandb-summary.json 249B/249B (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading config.yaml 2.2KB/2.2KB (0.4s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading wandb-summary.json 249B/249B (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading config.yaml 2.2KB/2.2KB (0.4s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading wandb-summary.json 249B/249B (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading config.yaml 2.2KB/2.2KB (0.4s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading wandb-summary.json 249B/249B (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading config.yaml 2.2KB/2.2KB (0.4s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading wandb-summary.json 249B/249B (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading config.yaml 2.2KB/2.2KB (0.4s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading wandb-summary.json 249B/249B (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading config.yaml 2.2KB/2.2KB (0.4s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading wandb-summary.json 249B/249B (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading config.yaml 2.2KB/2.2KB (0.4s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading wandb-summary.json 249B/249B (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading config.yaml 2.2KB/2.2KB (0.4s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               bert_score ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: bert_score_deberta-large ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   rouge1 ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   rouge2 ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   rouge3 ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   rouge4 ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   rougeL ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     sari ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    words ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               bert_score 83.23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: bert_score_deberta-large 52.46\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   rouge1 34.81\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   rouge2 10.45\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   rouge3 3.75\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   rouge4 1.82\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   rougeL 22.31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     sari 41.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    words 126.75\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mcrimson-sun-16\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/naveenkcb-university-of-illinois-urbana-champaign/mimic-iv-note-di-bhc_Llama-2-7b-hf_evaluation/runs/16zi69cm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/naveenkcb-university-of-illinois-urbana-champaign/mimic-iv-note-di-bhc_Llama-2-7b-hf_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250430_204347-16zi69cm/logs\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "%%shell\n",
        "cd /content/drive/MyDrive/DL4H-Project/patient_summaries_with_llms\n",
        "python summarization/fine_tune_llama_3.py --model_name_or_path meta-llama/Llama-2-7b-hf --evaluation --evaluation_model_path /content/drive/MyDrive/DL4H-Project/mimic-iv-note-di-bhc/models/Llama-2-7b-hf/mimic-iv-note-di-bhc_Llama-2-7b-hf_4000_600_chars_100_valid/lora_rank_8_lora_alpha_8_lora_dropout_0.05_num_target_modules_2_learning_rate_2e-5/best_val_loss --data_path /content/drive/MyDrive/DL4H-Project/mimic-iv-note-di-bhc/dataset --output_path           /content/drive/MyDrive/DL4H-Project/mimic-iv-note-di-bhc/models/Llama-2-7b-hf/mimic-iv-note-di-bhc_Llama-2-7b-hf_4000_600_chars_100_valid/lora_rank_8_lora_alpha_8_lora_dropout_0.05_num_target_modules_2_learning_rate_2e-5_test --num_train_examples 8 --num_val_examples 4 --num_test_examples 4\n"
      ]
    }
  ]
}