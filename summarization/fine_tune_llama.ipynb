{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01913805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17135eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fed932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig #nc42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b1a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "        get_peft_model, \n",
    "        prepare_model_for_kbit_training, \n",
    "        LoraConfig\n",
    "    )\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "from rouge_score import rouge_scorer\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf9dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Parameter tuning of LLM on mimic summarization.\")\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\", type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_path\", type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_path\", type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\", type=str, default=\"cuda\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--evaluation\", action=\"store_true\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--evaluation_model_path\", type=str, default=None\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # General parameters\n",
    "    parser.add_argument(\n",
    "        \"--num_train_examples\", type=int, default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_val_examples\", type=int, default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_test_examples\", type=int, default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_steps\", type=int, default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_and_logging_steps\", type=int, default=10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", type=int, default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\", type=int, default=16\n",
    "    )\n",
    "    \n",
    "    # Parameters to tune\n",
    "    parser.add_argument(\n",
    "        \"--lora_rank\", type=int, default=8\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lora_alpha\", type=int, default=8\n",
    "    )   \n",
    "    parser.add_argument(\n",
    "        \"--lora_dropout\", type=float, default=0.1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_target_modules\", type=int, default=4\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\", type=float, default=5e-4\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4cc293",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Use custom rouge function to obtain rouge 3/4 which are not available in huggingface\n",
    "def get_rouge_score(gold, pred):\n",
    "    rouge_scores = ['rouge1', 'rouge2', 'rouge3', 'rouge4', 'rougeL']\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_scores, use_stemmer=True)\n",
    "    scores = scorer.score(gold, pred)\n",
    "    return {k: scores[k].fmeasure * 100 for k in rouge_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e9001b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_custom_metrics(srcs, golds, preds, device):\n",
    "    scores = defaultdict(list)\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    sari = evaluate.load(\"sari\")\n",
    "    \n",
    "    # For rouge and length go over examples one by one and determine mean\n",
    "    for gold, pred in zip(golds, preds):\n",
    "        for k, v in get_rouge_score(gold, pred).items():\n",
    "            scores[k].append(v)\n",
    "        scores['words'].append(len(pred.split(' ')))\n",
    "    for k, v in scores.items():\n",
    "        scores[k] = np.mean(v)\n",
    "\n",
    "    # This is the default call using model_type=\"roberta-large\"\n",
    "    # This is the same as in the paper \"Generation of Patient After-Visit Summaries to Support Physicians\" (AVS_gen/eval_summarization.py) using the libary SummerTime\n",
    "    scores['bert_score'] = np.mean((bertscore.compute(predictions=preds, references=golds, lang=\"en\", device=device))['f1']) * 100\n",
    "    # BERTScore authors recommend \"microsoft/deberta-large-mnli\" (https://github.com/Tiiiger/bert_score)\n",
    "    scores['bert_score_deberta-large'] = np.mean((bertscore.compute(predictions=preds, references=golds, device=device, model_type=\"microsoft/deberta-large-mnli\"))['f1']) * 100\n",
    "    scores['sari'] = sari.compute(sources=srcs, predictions=preds, references=[[g] for g in golds])['sari']\n",
    "    # scores['sari'] = scores['sari'][0]\n",
    "    # Importing readability for dallc score not working: https://pypi.org/project/py-readability-metrics/    \n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fa5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_as_latex(metrics):\n",
    "    # Print latex table row\n",
    "    order = ['rouge1', 'rouge2', 'rouge3', 'rouge4', 'rougeL', 'bert_score', 'bert_score_deberta-large', 'sari', 'words']\n",
    "    print(' & '.join([f'${metrics[k]:.2f}$' for k in order]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f74ea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Code based on summarization mimic iv notebook\n",
    "    args = parse_args()\n",
    "    output_dir = args.output_path\n",
    "    \n",
    "    print('data:', args.data_path)\n",
    "    \n",
    "    # Load device\n",
    "    # device = args.device  # \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # For manual run with CUDA_VISIBLE_DEVICES use \"cuda:0\" as device\n",
    "    device = \"cuda\"\n",
    "    \n",
    "    # Log parameter tuning\n",
    "    target_modules = {1: ['q_proj'], 2: ['q_proj', 'v_proj'], 4: ['q_proj', 'k_proj', 'v_proj', 'o_proj']}\n",
    "    lora_config = LoraConfig(\n",
    "        r=args.lora_rank,\n",
    "        # Scaling lora weight matrix by alpha/r (in paper: used equals r, helps to return hyperparams when varying r)\n",
    "        # So effectively tuning alpha similar to tuning LR\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        target_modules=target_modules[args.num_target_modules],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    # Register wandb\n",
    "    # To disable uploading of output.log (possible information leakage)\n",
    "    # os.environ[\"WAND_CONSOLE\"] = \"off\"\n",
    "    # os.environ[\"WANDB_IGNORE_GLOBS\"] = \"output.log\"\n",
    "    short_model_name = args.model_name_or_path.split('/')[-1]\n",
    "    if args.evaluation:\n",
    "        wandb.init(project=f\"mimic-iv-note-di-bhc_{short_model_name}_evaluation\", config=args, tags=[], notes=\"\")\n",
    "    else:\n",
    "        wandb.init(project=f\"mimic-iv-note-di-bhc_{short_model_name}_parameter-tuning\", config=args, tags=[], notes=\"\")\n",
    "\n",
    "\n",
    "    #nc42 - load in 8 bit deprecated\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True   # or \n",
    "            #load_in_4bit=True, # perform 4-bit quantization to fit within 8-10GB RAM\n",
    "            #llm_int8_threshold=6.0  # Optional: for better performance on some models\n",
    "    )\n",
    "\n",
    "\n",
    "    # Load model\n",
    "    hf_token = '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'\n",
    "    model_name = args.model_name_or_path\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                #load_in_8bit=True, #nc42\n",
    "                                                quantization_config=bnb_config,  #nc42\n",
    "                                                # For manual run with CUDA_VISIBLE_DEVICES use \"cuda:0\" as device_map\n",
    "                                                device_map='auto',  # can be specific with device, but if CUDA_VISIBLE_DEVICES is set, auto should work\n",
    "                                                token=hf_token,\n",
    "                                                )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)  # Padding size for batched prediction from HF warning\n",
    "    # Workaround for missing padding token\n",
    "    # From https://github.com/huggingface/transformers/issues/22312 (comment Jun 28)\n",
    "    # tokenizer.pad_token='[PAD]' \n",
    "    # assert tokenizer.pad_token_id == 0\n",
    "    \n",
    "    \n",
    "    # Read data\n",
    "    data_path = args.data_path\n",
    "\n",
    "    data_files = {}\n",
    "    extension = \"json\"\n",
    "    # Model trained on shortened texts and summaries\n",
    "    data_files[\"train\"] = data_path + '/train_4000_600_chars_251-350_pt.json'\n",
    "    data_files[\"validation\"] = data_path + '/valid_4000_600_chars.json'\n",
    "    data_files[\"test\"] = data_path + '/valid_4000_600_chars.json'\n",
    "    # For testing\n",
    "    # data_files[\"test\"] = data_path + '/test_4000_600_chars_last_100.json'\n",
    "\n",
    "    # Model trained on full texts and summaries\n",
    "    # data_files[\"train\"] = data_path + '/train.json'\n",
    "    # data_files[\"validation\"] = data_path + '/valid.json'\n",
    "    # data_files[\"test\"] = data_path + '/valid.json'\n",
    "    \n",
    "    print(f\"Loading data from {data_files}\")\n",
    "    data = load_dataset(extension, data_files=data_files)\n",
    "\n",
    "    # data = load_dataset(\"samsum\")\n",
    "    data_train, data_test, data_val = data[\"train\"], data[\"test\"], data[\"validation\"]\n",
    "\n",
    "    # Limit number of examples\n",
    "    # Take examples from the end to prevent data leake with experiments done on beginning examples\n",
    "    if args.num_train_examples:\n",
    "        data_train = data_train.select(range(0, len(data_train))[-args.num_train_examples:])\n",
    "    if args.num_val_examples:\n",
    "        data_val = data_val.select(range(0, len(data_val))[-args.num_val_examples:])\n",
    "    if args.num_test_examples:\n",
    "        data_test = data_test.select(range(0, len(data_test))[-args.num_test_examples:])\n",
    "    \n",
    "    print(f\"Number of training examples: {len(data_train)}\")\n",
    "    print(f\"Number of validation examples: {len(data_val)}\")\n",
    "    print(f\"Number of test examples: {len(data_test)}\")\n",
    "\n",
    "    # # Include ':' for texts to exclude ':' from being treated as labels to train on\n",
    "    # # Because using these texts later for the collator\n",
    "    \n",
    "    # Default\n",
    "    instruction_text = \"Summarize for the patient what happened during the hospital stay based on this doctor's note:\"\n",
    "    response_text = \"Summary for the patient:\"\n",
    "    \n",
    "    # Experiment (prompt 2 from slide)\n",
    "    # instruction_text = \"Summarize this clinical note for the patient in simple English. Only use the information provided in the clinical note itself and general medical knowledge or advice.\\n\\nClinical note:\"\n",
    "    # response_text = \"Patient summary:\"\n",
    "\n",
    "    def generate_prompt(reference, summary=None, eos_token=\"</s>\"):\n",
    "        # Default\n",
    "        instruction = f\"{instruction_text}\\n\"\n",
    "        input = f\"{reference}\\n\"\n",
    "        \n",
    "        # New\n",
    "        # instruction = f\"{instruction_text} \"\n",
    "        # input = f\"{reference}\\n\\n\"\n",
    "\n",
    "        response = f\"{response_text} {summary + ' ' + eos_token if summary else ''} \"\n",
    "        return ''.join([instruction, input, response])\n",
    "    \n",
    "    def truncate_text(example, tokens=4096):\n",
    "        # Crop sentences of the text at the end until the prompt has less than 4096 tokens\n",
    "        # This is to avoid truncation of the prompt\n",
    "        example['truncated'] = False\n",
    "        while tokenizer(generate_prompt(example[\"text\"], example[\"summary\"]), return_tensors=\"pt\")[\"input_ids\"].shape[1] >= tokens:\n",
    "            example[\"text\"] = example[\"text\"].rsplit('.', 1)[0]\n",
    "            example['truncated'] = True\n",
    "        return example\n",
    "        \n",
    "    # Truncate texts\n",
    "    data_train = data_train.map(truncate_text)\n",
    "    data_val = data_val.map(truncate_text)\n",
    "    data_test = data_test.map(truncate_text)\n",
    "    # Check how manyt texts are truncated\n",
    "    print(f\"Truncated texts: {sum(data_train['truncated'])} train, {sum(data_val['truncated'])} val, {sum(data_test['truncated'])} test\")\n",
    "    \n",
    "    print(\"Prompted train example:\")\n",
    "    print(generate_prompt(data_train[0][\"text\"], data_train[0][\"summary\"]))\n",
    "    print()\n",
    "    print(\"Prompted valid example:\")\n",
    "    print(generate_prompt(data_val[0][\"text\"], data_val[0][\"summary\"]))\n",
    "\n",
    "\n",
    "    # Prediction method\n",
    "    def predict(prediction_model, dataset):\n",
    "        predictions = []\n",
    "\n",
    "        for ex in dataset:\n",
    "            input_prompt = generate_prompt(ex[\"text\"])\n",
    "            input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                generation_output = prediction_model.generate(\n",
    "                    input_ids=input_tokens,\n",
    "                    # 98% CI of summaries is 322 tokens, 99% CI is 370 tokens\n",
    "                    max_new_tokens=350,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "            prediction = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "            prediction = prediction[len(input_prompt):].strip()\n",
    "            predictions.append(prediction)\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "    # Predict validation examples and compute metrics\n",
    "    # predictions_val = predict(data_val)\n",
    "    # compute_custom_metrics(data_val[\"text\"], data_val[\"summary\"], predictions_val)\n",
    "\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    if args.evaluation:\n",
    "        print(f\"Loading model for evaluation: {args.evaluation_model_path}\")\n",
    "        trained_model = PeftModel.from_pretrained(model, args.evaluation_model_path, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "    else: \n",
    "        # Training\n",
    "        # Loading in 8 bit ...\"\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        \n",
    "\n",
    "        # Training setup\n",
    "        training_args = transformers.TrainingArguments(\n",
    "                    # To obtain decoded generations in compute_metrics one can use predict_with_generate=True for Seq2SeqArguments, but here no effect\n",
    "                    # See used in summarization example and then possible to get rouge in compute_metrics: https://huggingface.co/docs/transformers/tasks/summarization\n",
    "                    output_dir=output_dir,\n",
    "                    per_device_train_batch_size=args.batch_size, \n",
    "                    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "                    per_device_eval_batch_size=args.batch_size,\n",
    "                    eval_accumulation_steps=args.gradient_accumulation_steps,\n",
    "                    evaluation_strategy=\"steps\",\n",
    "                    max_steps=args.max_steps,\n",
    "                    save_steps=args.save_and_logging_steps, # for laod_best_model_at_end save_steps must be multiple of eval_steps\n",
    "                    logging_steps=args.save_and_logging_steps, # eval_steps defaults to this\n",
    "                    load_best_model_at_end=True,\n",
    "                    # metric_for_best_model defaults to loss, rouge not possible since no decoding in compute_metrics\n",
    "                    optim = \"paged_adamw_32bit\",\n",
    "                    # no clear evidence for lr scheduler:\n",
    "                    # * original llama and some sources use cosine (however llama with minimum of 0.1)\n",
    "                    # * often when paged_adamw is used, constant is used (strongest evidence: https://www.philschmid.de/instruction-tune-llama-2)\n",
    "                    lr_scheduler_type=\"constant\",\n",
    "                    learning_rate=args.learning_rate,\n",
    "                    max_grad_norm=0.3,\n",
    "                    warmup_ratio=0.03,\n",
    "                    group_by_length=True,\n",
    "                    ddp_find_unused_parameters=False,\n",
    "                    report_to=\"wandb\",\n",
    "                )\n",
    "\n",
    "\n",
    "        # Training\n",
    "        def formatting_func(example):\n",
    "            output = []\n",
    "            for t, s in zip(example[\"text\"], example[\"summary\"]):\n",
    "                prompt = generate_prompt(t, s)\n",
    "                output.append(prompt)\n",
    "            return output\n",
    "\n",
    "        response_template_with_context = '\\n' + response_text  # We added context here: \"\\n\". This is enough for this tokenizer\n",
    "        response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  # Now we have it like in the dataset texts\n",
    "        data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            train_dataset=data_train,\n",
    "            eval_dataset=data_val,\n",
    "            peft_config=lora_config,\n",
    "            formatting_func=formatting_func,\n",
    "            max_seq_length=4096, # 1024 so far, important to set high enough to avoid truncation\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            # compute_metrics=compute_custom_metrics, # compute_metrics only get logits but no decoding and choosing argmax gives wrong results\n",
    "            # Consider adding NEFTune here\n",
    "            # neftune_noise_alpha=5,\n",
    "            args=training_args,\n",
    "        )\n",
    "\n",
    "        # We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "        for name, module in trainer.model.named_modules():\n",
    "            if \"norm\" in name:\n",
    "                module = module.to(torch.float32)\n",
    "\n",
    "        trainer.train()\n",
    "        trainer.save_model(f\"{output_dir}/best_val_loss\") \n",
    "        trained_model = trainer.model\n",
    "    \n",
    "    # Get validation score\n",
    "    # print(\"\\nPredicting validation examples.\")\n",
    "    # predictions_val = predict(trained_model, data_val)\n",
    "    # metrics_val = compute_custom_metrics(data_val[\"text\"], data_val[\"summary\"], predictions_val, device)\n",
    "    # print(\"Validation metrics:\")\n",
    "    # print(metrics_val)\n",
    "    \n",
    "    print(\"\\nPredicting test examples.\")\n",
    "    predictions_test = predict(trained_model, data_test)\n",
    "    metrics_test = compute_custom_metrics(data_test[\"text\"], data_test[\"summary\"], predictions_test, device)\n",
    "    # Print test examples text and summary fields\n",
    "    print(\"Test examples:\")\n",
    "    for i in range(min(len(data_test), 10)):\n",
    "        print(f\"\\n\\n\\nExample {i}:\\n\\n{data_test[i]['text']}\\n\\n{data_test[i]['summary']}\\n\\n{predictions_test[i]}\")\n",
    "        \n",
    "    # Store predictions into jsonl file as dict with keys: summary\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_path / 'predictions_test.jsonl', \"w\") as f:\n",
    "        f.write(\"\\n\".join(predictions_test))\n",
    "\n",
    "    with open(output_path / 'predictions_test_dict.jsonl', \"w\") as f:\n",
    "        for pred in predictions_test:\n",
    "            f.write(f'{{\"summary\": \"{pred}\"}}\\n')\n",
    "\n",
    "    print(\"Test metrics:\")\n",
    "    print(metrics_test)\n",
    "    metrics_test = {k: round(v, 2) for k, v in metrics_test.items()}\n",
    "    wandb.log(metrics_test)\n",
    "    \n",
    "    print(\"Test metrics rounded:\")\n",
    "    print(metrics_test)\n",
    "    print_metrics_as_latex(metrics_test)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f70e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
